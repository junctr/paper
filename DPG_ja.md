決定論的政策勾配アルゴリズム

Abstract

本論文では，連続的な行動を持つ強化学習のための決定論的政策勾配アルゴリズムについて考察する．決定論的政策勾配は特に魅力的な形式を持っている：それは行動-値関数の期待勾配である．この単純な形式は，通常の確率的政策勾配よりもはるかに効率的に決定論的政策勾配を推定することができることを意味する．十分な探索を保証するために，探索的行動政策から決定論的目標政策を学習する政策外行動者批評アルゴリズムを導入する．我々は、決定論的政策勾配アルゴリズムが、高次元行動空間において、確率的対応策を著しく凌駕することを実証する。　

1. introduction

政策勾配アルゴリズムは、連続的な行動空間を持つ強化学習問題において広く用いられている。基本的な考え方は、パラメータベクトルθに従って状態sにおける行動aを確率的に選択するパラメトリック確率分布 πθ(a|s) = P [a|s; θ] によって政策を表現することである。政策勾配アルゴリズムは通常、この確率的政策をサンプリングし、累積報酬が大きくなる方向に政策パラメータを調整することで進行する。

本論文では、代わりに決定論的政策 a = µθ(s) を考える。このとき，確率的政策と同じように，政策勾配の方向に政策パラメータを調整することができるかどうかを考えるのは自然なことである．以前は，決定論的な政策勾配は存在しないか，モデルを用いた場合にのみ得られると考えられていた(Peters, 2010)．しかし，我々は決定論的な政策勾配が実際に存在すること，さらにそれが単純に行動-価値関数の勾配に従うという，モデルに依存しない単純な形式を持つことを示す．さらに，決定論的政策勾配は，政策分散がゼロになるにつれて，確率的政策勾配の極限的な場合であることを示す．

実用的な観点からは，確率的政策勾配と決定論的政策勾配の間には決定的な違いがある．確率的な場合，政策勾配は状態空間と行動空間の両方を積分するのに対し，決定論的な場合，状態空間のみを積分する．その結果，確率的政策勾配を計算する場合，特に行動空間が多次元である場合，より多くのサンプルを必要とすることがある．状態空間と行動空間を完全に探索するためには，確率的な政策が必要となる場合が多い．決定論的な政策勾配アルゴリズムが満足のいく探索を続けられるようにするために、我々はオフポリシー学習アルゴリズムを導入する。基本的な考え方は、（十分な探索を保証するために）確率的な行動政策に従って行動を選択するが、（決定論的政策勾配の効率性を利用して）決定論的な目標政策について学習することである。我々は決定論的政策勾配を用いて、微分可能な関数近似器を用いて行動値関数を推定し、近似的行動値勾配の方向に政策パラメータを更新するオフポリシーアクタークリティックアルゴリズムを導出する。また、決定論的な政策勾配に対して互換性のある関数近似の概念を導入し、近似が政策勾配に偏りを与えないことを保証する。

我々は、高次元バンディット、低次元の行動空間を持ついくつかの標準的な強化学習課題、およびタコの腕を制御するための高次元課題といったいくつかのベンチマーク問題に対して、我々の決定論的行為者批評アルゴリズムを適用した。その結果、特に高次元課題において、確率的政策勾配よりも決定論的政策勾配を用いた方が性能的に有利であることが示された。さらに、我々のアルゴリズムは、従来の方法と比較して、計算量を必要としない。各更新の計算量は、行動の次元と政策パラメータ数に対して線形である。最後に、微分可能な制御方針は提供されているが、コントローラにノイズを注入する機能がないようなアプリケーション（例えば、ロボット工学）が多く存在する。このような場合、確率的な政策勾配は適用できないが、我々の方法はまだ有用である可能性がある。

2. background

2.1. preliminaries

我々は、エージェントが確率的環境において、累積報酬を最大化するために、一連の時間ステップにわたって行動を逐次選択することによって行動する強化学習と制御問題を研究している。状態空間S、行動空間A、密度p1(s1)を持つ初期状態分布、マルコフ特性を満たす条件密度p(st+1|st, at)を持つ定常遷移ダイナミクス分布からなるマルコフ決定過程（MDP）としてこの問題をモデル化する。 MDPにおける行動の選択には、政策が用いられる。一般に政策は確率的であり、πθ : S → P(A) で示される。ここで、P(A) は A 上の確率測定の集合、θ∈R n は n 個のパラメータのベクトルであり、πθ (at|st) は政策と関連する at における条件付き確率密度である。エージェントは政策を用いてMDPと対話し，S×A×R上の状態，行動，報酬の軌跡 h1:T = s1, a1, r1..., sT , aT , rTを与える．価値関数は、期待総割引報酬、V π (s) = E [r γ 1 |S1 = s; π] および Qπ (s, a) = E [r γ 1 |S1 = s, A1 = a; π]として定義されている。1 エージェントの目標は、開始状態からの累積割引報酬を最大化する政策を得ることであり、性能目標 J(π) = E [r γ 1 |π]で示される。状態 s から t 時間ステップで遷移した後の状態 s 0 での密度を p(s → s 0 , t, π) と表す．

また、（不適切な）割引状態分布を ρ π (s 0 ) := R S P∞ t=1 γ t-1p1(s)p(s → s 0 , t, π)ds で表す。そして、性能目標を期待値として、
(1) 

ここでEs∼ρ [-] は割引状態分布 ρ(s) に関する（不適切）期待値であることを表す。2 以下、簡単のため、A = R m、S は R d のコンパクト部分集合であるとする。

2.2. stochastic policy gradient theorem

政策勾配アルゴリズムは、おそらく連続行動強化学習アルゴリズムの中で最も人気のあるクラスである。これらのアルゴリズムの基本的な考え方は、性能勾配∇θJ(πθ)の方向にポリシーのパラメータθを調整することである。これらのアルゴリズムの基礎となる基本結果は、政策勾配定理 (Sutton et al., 1999) である、
(2) 

政策勾配は驚くほど簡単である。特に、状態分布ρ π (s) が政策パラメータに依存するにもかかわらず、政策勾配は状態分布の勾配に依存しない。この定理は，性能勾配の計算を単純な期待値に還元するものであり，重要な実用的価値を持つ．政策勾配の定理は，この期待値のサンプルベースの推定値を形成することにより，様々な政策勾配アルゴリズム (Degris et al., 2012a) を導き出すために使用されてきた．これらのアルゴリズムが対処しなければならない問題の一つは，行動値関数Qπ (s, a)をどのように推定するかということである．おそらく最も単純なアプローチは、サンプルリターンr γ tを使用してQπ（st，at）の値を推定することであり、これはREINFORCEアルゴリズム（Williams，1992）の変種につながるものである。

2.3. stochastic actor-critic algorisms

Actor-critic は政策勾配定理に基づくアーキテクチャとして広く用いられている (Sutton et al., 1999; Peters et al., 2005; Bhatnagar et al., 2007; Degris et al., 2012a)．アクター・クリティックは、2つの同名のコンポーネントから構成される。行為者は確率的政策πθ(s)のパラメータθを式2の確率的勾配上昇によって調整する。評論家は時間差学習などの適切な政策評価アルゴリズムを用いて行動値関数 Qw(s, a) ≈ Qπ(s, a) を推定する．一般に、真の行動値関数Qπ (s, a)を関数近似値Qw (s, a)に置き換えることは、バイアスをもたらす可能性がある。しかし、Qw(s, a) = ∇θ log πθ(a|s) >w となるような関数近似が適合し、かつ、平均二乗誤差2 (w) = Es∼ρπ,a∼πθ h (Qw(s, a) - Qπ(s, a))2 i を最小化するパラメータwが選ばれるならば、偏りはない (Sutton et al.1999)。1999), 
(3) 

より直観的には、条件i)は、互換関数近似器が確率政策の「特徴」に対して線形であることを述べています。∇θ log πθ(a|s), 条件ii) は、これらの特徴からQπ (s, a)を推定する線形回帰問題の解をパラメータとすることを要求しています。実際には，時間差学習によってより効率的に価値関数を推定する政策評価アルゴリズムのために，条件ii)は通常緩和される(Bhatnagar et al., 2007; Degris et al., 2012b; Peters et al., 2005); 実際にi)とii)が両方満たされれば，全体のアルゴリズムはreinforceアルゴリズムと同様に，評論家を全く使用しないと同等となる (Sutton et al., 2000) (Williams, 1992)．

2.4. off-policy actor-critic

このような場合、明確な行動政策β(a|s) 6= πθ(a|s) からサンプリングした軌道から政策勾配をオフポリシーで推定することが有用であることが多い。オフポリシー設定において，性能目標は，一般に，行動ポリシーの状態分布に平均化された目標ポリシーの価値関数に修正される (Degris et al., 2012b), Jβ(πθ) = Z S ρ β (s)V π (s)ds = Z S Z A ρ β (s)πθ(a|s)Qπ (s, a)dads 
パフォーマンス目標を差分化して近似するとオフポリシー政策勾配になる (Degris et al., 2012b)．2012b）
（4）
 (5) 

この近似では、作用値勾配に依存する項∇θQπ (s, a)を削除している。Degris et al. (2012b)は、勾配が収束する局所最適値の集合を保持することができるので、これは良い近似であると主張している。Off-Policy Actor-Critic (OffPAC) アルゴリズム (Degris et al., 2012b) は行動政策 β(a|s) を用いて軌道を生成する．批評家はこれらの軌道から勾配時間差学習 (Sutton et al., 2009) によりオフポリシー状態値関数 V v (s) ≈ V π (s) を推定する．また，アクタは政策パラメータθを，式5の確率的勾配上昇により，これらの軌道から政策的に外れた状態で更新する．このとき，式 5 の未知の行動値関数 Qπ (s, a) の代わりに時間差誤差 δt が用いられ，δt = rt+1 + γV v (st+1) - V v (st); これが真の勾配の近似となることが示される (Bhatnagar et al., 2007)．また、重要度サンプリング比率 πθ(a|s) βθ(a|s) を用いて、行為者が β ではなく π に従って選択されたことを補正している。

3. gradients of deterministic policies

ここで、政策勾配の枠組みが決定論的政策にどのように拡張されるかを考える。我々の主要な結果は決定論的政策勾配定理であり，前節で示した確率的政策勾配定理に類似している．この結果を導き出し、理解するためのいくつかの方法を提供する。まず、決定論的政策勾配の形式の背後にある非公式な直観を提供する。次に，第一原理から決定論的政策勾配の定理を正式に証明する．最後に、決定論的政策勾配定理が、実は確率的政策勾配定理の限定的な場合であることを示す。証明の詳細は付録までおあずけ。

3.1. action-value gradients

モデルフリー強化学習アルゴリズムの大部分は，一般化された政策反復に基づくものであり，政策評価と政策改良のインターリーブを行っている (Sutton and Barto, 1998)．政策評価法は，行動価値関数Qπ (s, a) またはQμ (s, a) を，例えばモンテカルロ法や時間差学習によって推定する．政策改善法は，（推定された）行動価値関数にしたがって政策を更新する．最も一般的なアプローチは，行動値関数の貪欲な最大化（またはソフトな最大化）， μ k+1(s) = argmax Q a μ k (s, a)である．

連続的な行動空間では，貪欲な政策改善は問題となり，すべてのステップで大域的な最大化を必要とする．具体的には，各状態 s に対して，政策パラメータ θ k+1 を勾配 ∇θQµ k (s, µθ(s)) に比例して更新する．各状態はそれぞれ異なる政策改善の方向を示す．これらは，状態分布ρμ(s)に関する期待値をとることによって平均化される．
(6) 

連鎖法則を適用すると，政策改善は行動に関する行動値の勾配，および政策パラメータに関する政策の勾配に分解できることが分かる．
(7) 

慣例により，∇θµθ(s)は，各列が政策パラメータθに対する政策のd番目の行動次元の勾配∇θ［µθ(s)］dとなるヤコビアン行列とする． 

しかし，政策を変更することによって，異なる状態を訪問し，状態分布ρμは変更することになる．その結果、分布への変化を考慮せずに、このアプローチで改善が保証されることはすぐには理解できない。しかし，以下の理論により，確率的政策勾配定理と同様に，状態分布の勾配を計算する必要はなく，上記の直感的な更新は，まさに性能目標の勾配に従うものであることが示される．

3.2. deterministic policy gradient theorem

性能目標J(μθ) = E [r γ 1 |µ] を定義し、確率分布p(s → s 0 , t, µ) と割引状態分布ρ µ(s) を確率的な場合と同様に定義する。これによっても、性能目標を期待値として、
(8) 

ここで、政策勾配定理の決定論的アナローグを提供する。証明は(Sutton et al., 1999)と同様の方式で行い、付録Bに示す。

定理1 (Deterministic Policy Gradient Theorem).
MDPが条件A.1（付録参照；これらは∇θμθ（s）と∇aQμ（s，a）が存在し、決定論的政策勾配が存在することを意味する）を満たしているとする。すると，
(9)

3.3. limit of the stochastic policy gradient

決定論的政策勾配定理は一見すると確率論的政策勾配（式2）と似て非なるものである．しかし、多くのバンプ関数を含む広いクラスの確率的政策に対して、決定論的政策勾配は確かに確率的政策勾配の特別な（限界）ケースであることを、これから示そう。確率的政策πμθ,σを決定論的政策μθ : S → Aと分散パラメータσでパラメタライズし、σ = 0において確率的政策が決定論的政策と等価であるπμθ,0 ≡μθを示す。そして、σ → 0として、確率的政策の勾配が決定論的勾配に収束することを示す（証明と技術的条件については付録Cを参照）。

定理2．πμθ,σ（a|s）＝νσ（μθ（s），a）であり、σは分散を制御するパラメータ、νσは条件B・1を満たし、MDPは条件A・1及びA・2を満たすような確率的政策πμθ,σを考える。すると、
(10) 

ここで、l.h.s.上の勾配は標準確率政策勾配、r.h.s.上の勾配は決定論政策勾配である。これは，政策勾配によく使われる，互換関数近似 (Sutton et al., 1999), 自然勾配 (Kakade, 2001), Actor-critic (Bhatnagar et al., 2007), エピソード/バッチ法 (Peters et al., 2005) が決定論的政策勾配に適用できることを示しており，重要な結果となっています．

4. deterministic actor-critic

我々は決定論的政策勾配定理を用いて、政策上と政策外の行為者批判アルゴリズムを導出する。まず最も単純なケースとして、単純なSarsa評論家を用いたオンポリシー更新から始め、できるだけ明確にアイデアを説明する。次に、オフポリシーの場合について、今度は単純なQ-learning評論家を用いて、重要なアイデアを説明する。これらの単純なアルゴリズムは、関数近似器によってもたらされるバイアスと、オフポリシー学習によって引き起こされる不安定性の両方によって、実際には収束の問題がある可能性があります。次に、互換性のある関数近似と勾配時間差学習を用いた、より原理的なアプローチに移る。

4.1. on-policy deterministic actor-critic

一般に，決定論的な政策に従って振舞うことは，十分な探索を保証するものではなく，最適でない解を導く可能性がある．それにもかかわらず、我々の最初のアルゴリズムは、決定論的なポリシーを学習し、それに従うオンポリシーアクタークリティックアルゴリズムである。その主な目的は教則的である。しかし、決定論的な行動方針であっても、十分な探索を確保するために環境に十分なノイズが存在するような環境では有用であろう。

確率的行動評論家と同様に、決定論的行動評論家は 2 つの要素から構成される。批評家は行動価値関数を推定し、行為者は行動価値関数の勾配を上昇させる。具体的には，アクターは決定論的政策 µθ(s) のパラメータ θ を，式 9 の確率的勾配上昇によって調整する．確率的行動者批判と同様に、真の行動価値関数 Qµ(s, a) の代わりに微分可能な行動価値関数 Qw(s, a) を代入する。評論家は適切な政策評価アルゴリズムを用いて行動価値関数 Qw(s, a) ≒ Qµ(s, a) を推定する．例えば，以下の決定論的行為者批判アルゴリズムでは，批判者はサルサ更新を用いて行為価値関数を推定する(Sutton and Barto, 1998），
(11,12,13)

4.2. off-policy deterministic actor-critic

ここで，任意の確率的な行動政策π(s, a)によって生成された軌道から決定論的な目標政策µθ(s)を学習するオフポリシー法について考察する．前回と同様に、性能目標を、行動ポリシーの状態分布に平均化された目標ポリシーの価値関数である
(14) 
(15) 

この方程式によりオフポリシーの確定的政策勾配を得ることができます．確率的な場合（式4参照）と同様に，∇θQμθ（s，a）に依存する項を削除した．この近似を支持するDegrisら（2012b）と同様の正当化が可能である．

次に，政策外の決定論的政策勾配の方向へ政策を更新する行為者批判アルゴリズムを開発する．式15において，真の行動価値関数Qμ（s，a）の代わりに微分可能な行動価値関数Qw（s，a）を再び代用する．評論家は，適切な政策評価アルゴリズムを用いて，β(a|s)によって生成された軌道から政策外で行動値関数Qw(s, a) ≈ Qµ(s, a)を推定する．以下の政策外決定論的アクタークリティック (OPDAC) アルゴリズムでは、クリティックは Q 学習更新を用いて行動-価値関数を推定する。
(16,17,18)

確率的政策外行為批判アルゴリズムでは通常行為者と批判者が共に重要度サンプリングを使っていることに注意する (Degris et al., 2012b)．しかし，決定論的政策勾配は行動に関する積分を除去するため，アクターにおける重要度サンプリングを回避することができる．

4.3. compatible function approximation

一般に、決定論的な政策勾配に近似 Qw(s, a) を代入しても、必ずしも真の勾配に従うとは限りません（実際、必ずしも全く上昇方向にはなりません）。確率的な場合と同様に、我々は真の勾配が保存されるような互換性のある関数近似器Qw(s, a)のクラスを見つけることができました。言い換えれば、決定論的な政策勾配に影響を与えることなく、勾配∇aQμ（s，a）を∇aQw（s，a）に置き換えることができるような近似器Qw（s，a）を見つけるのです。以下の定理は、オンポリシー、E[-] = Es∼ρµ [-]、オフポリシー、E[-] = Es∼ρβ [-]のいずれにも適用されます。

定理3．
関数近似器Qw（s，a）は，決定論的政策μθ（s）に適合し， ∇θJβ（θ）＝E h ∇θμθ（s） ∇aQw（s，a）｜a=μθ（s） i ，
1. ∇aQw（s，a）｜a=μθ（s）＝∇θμθ（s） >w かつ，
2．w は平均二乗誤差 MSE(θ, w) = E (s; θ, w) > (s; θ, w) ここで (s; θ, w) = ∇aQw(s, a)|a=µθ(s) - ∇aQµ(s, a)|a=µθ(s) Proof.を最小化する。

w が MSE を最小化するならば、2 w.r.t. w の勾配はゼロでなければならない。そこで、条件1により

任意の決定論的政策μθ(s)に対して，Qw(s, a) = (a - μθ(s))>∇θμθ(s) >w + V v (s) の形の互換関数近似器が常に存在し，V v (s) は行動aから独立した任意の微分可能な基底関数で，例えば状態の特徴φ（s）とパラメータvとの線形結合，パラメータvに対してはV v (s) = v >φ(s) となることがある．自然な解釈として、V v (s) は状態 s の値を推定し、第一項は状態 s において行動 µθ(s) より行動 a をとることの優位性 Aw(s, a) を推定するものである。この優位性関数は、状態アクション特徴量φ(s, a) def = ∇θµθ(s)(a - µθ(s)) とパラメータwによる線形関数近似器Aw(s, a) = φ(s, a) >w として見ることができる。なお、行動次元がm個、政策パラメータがn個ある場合、∇θμθ(s)はn×mのヤコビアン行列なので、特徴ベクトルはn×1、パラメータベクトルwもn×1となり、この形式の関数近似器は定理3の条件1を満足する。
なお、線形関数近似器は、アクション値が大きい場合、アクション値が±∞に発散するため、グローバルにアクション値を予測するためにはあまり有用でない。しかし、局所的な批評家としては依然として高い効果を発揮することができる。特に，現在の政策から逸脱した場合の局所的な利点を表すAw(s, μθ(s) + δ) = δ >∇θμθ(s) >w (δは決定論的政策からの小さな逸脱を表す)である．その結果，アクターが政策パラメータを調整すべき方向を選択するためには，線形関数近似で十分である．

条件2を満たすためには、Qwの勾配と真の勾配との間の平均二乗誤差を最小化するパラメータwを見つける必要がある。これは，「特徴量」φ（s, a）と「目標値」∇aQμ（s, a）｜a=μθ（s） を持つ線形回帰問題として捉えることができる．言い換えれば，政策の特徴は状態 s における真の勾配 ∇aQµ(s, a) を予測するために使用される．実際には，条件1を満たすために線形関数近似器Qw(s, a) = φ(s, a) >w を用いるが，条件2を正確に満たさない標準的な政策評価法（例えば，オンポリシーまたはオフポリシーの決定論的行為者批判アルゴリズムそれぞれに対するSarsaまたはQ-learning）により，wを学習させる．政策評価問題の妥当な解は，Qw(s, a) ≈ Qµ(s, a) を求め，したがって，(滑らかな関数近似の場合) ∇aQw(s, a)|a=µθ(s) ≈ ∇aQµ(s, a)|a=µθ(s) をほぼ満たすことに注意されたい．

要約すると、compatible off-policy deterministic actorcritic (COPDAC) アルゴリズムは2つの要素から構成されます。評論家は、特徴量φ(s, a) = a >∇θµθ(s) から行動値を推定する線形関数近似器である。これは例えば Q-learning や勾配 Qlearning を用いて、行動政策 β(a|s) のサンプルから政策外で学習されるかもしれない。そしてアクターは評論家の行動値勾配の方向にそのパラメータを更新する。以下の COPDAC-Q アルゴリズムは単純な Q-learning Critic を使用しています。
(19,20,21,22)

線形関数近似を用いた場合、オフポリシーQ学習が発散する可能性があることはよく知られている。しかし，最近の勾配時間差学習に基づく方法は，真の勾配降下アルゴリズムであり，確実に収束する(Sutton et al., 2009)．これらの手法の基本的な考え方は、確率的勾配降下により平均二乗投影ベルマン誤差（MSPBE）を最小化することであるが、詳細は本論文の範囲外である。OffPACアルゴリズム(Degris et al., 2012b)と同様に、我々は批評家において勾配時間差学習を用いる。具体的には、gradient Q-learningを用い（Maei et al., 2010）、ステップサイズαθ、αw、αuの適切な条件下で、criticがactorよりも速い時間スケールで更新されることを保証し、criticがMSBEを最小化するパラメータに収束することに注目する（Sutton et al., 2009; Degris et al., 2012b）。以下のCOPDAC-GQアルゴリズムは、COPDACと勾配Q学習評論家を組み合わせ、
(23,24,25,26,27)

確率的行為者批判アルゴリズムと同様に、これら全ての更新の計算量は時間ステップあたり O(mn) である。

最後に，自然政策勾配 (Kakade, 2001; Peters et al., 2005) が決定論的政策に拡張可能であることを示す．任意のメトリックM(θ)に関する我々の性能目標の最急上昇方向はM(θ) -1∇θJ(µθ) (Toussaint, 2012)によって与えられる．自然勾配はフィッシャー情報メトリックMπ() = Es∼ρπ,a∼πθ ∇θ log πθ(a|s)∇θ log πθ(a|s) > に関する最急上昇方向で，このメトリックは政策のパラメータ変更に不変である (Bagnell と Schneider, 2003)．決定論的政策については，Mμ(θ) = Es∼ρμ ∇θμθ(s)∇θμθ(s) > というメトリックを用いるが，これは政策の分散がゼロになるため，フィッシャー情報メトリックの極限ケースとみなすことができる．決定論的政策勾配定理と互換関数近似を組み合わせることにより、∇θJ(µθ) = Es∼ρµ ∇θµθ(s)∇θµθ(s) >wとなり、最急上昇方向は単にMμ（θ） -1∇θJβ（µθ） = wとなる。このアルゴリズムは式20または24を単純化するとθt+1 = θt +に実装可能である。

5. experiments

5.1. continuous bandit

dit 最初の実験では，確率的政策勾配と決定論的政策勾配を直接比較することに焦点を当てる．問題は高次元の二次コスト関数-r(a) = (a - a ∗ ) >C(a - a ∗ ) を持つ連続バンディット問題である．行列Cは{0.1, 1}から選ばれた固有値を持つ正定値であり、a ∗ = [4, ..., 4]>である。我々は作用次元をm = 10, 25, 50と考える。この問題は、2次関数に関する完全な知識があれば解析的に解くことができるが、ここでは、モデルフリーな確率的政策勾配アルゴリズムと決定論的政策勾配アルゴリズムの相対的性能に興味をもっている。

ms. バンディット課題(SAC-B)における確率的アクター・クリティックでは、等方性ガウスポリシー, πθ,y(-) ∼ N (θ, exp(y)) を用い、ポリシーの平均と分散の両方を適応させる。決定論的行為者批判アルゴリズムはCOPDACに基づき、目標政策、μθ = θと固定幅ガウス行動政策、β(-) ∼ N (θ, σ2 β ) を使用する。SAC-Bでは互換性のある特徴は∇θ log πθ(a); COPDAC-Bでは∇θμθ(a)(a - θ); 両者ともバイアス特徴も含まれている。この実験では、Criticは2mステップの各バッチから再計算され、アクターは各バッチで1回更新されます。性能を評価するために、平均で発生するステップごとの平均コストを測定する（すなわち、探索はオンポリシーアルゴリズムではペナルティがない）。我々は全てのステップサイズパラメータと分散パラメータ（SACは初期値y、COPDACはσ2β）に対してパラメータスイープを行った。図1は、各アルゴリズムで最も性能の良いパラメータを5回実行し、平均した性能です。この結果から、次元が大きくなるにつれて大きくなる決定論的更新に大きな性能上の優位性があることがわかる。また、確率的アクタークリティックが決定論的アクタークリティックと同じ固定分散σ 2 βを使用し、平均のみが適応されるようにした実験も実施した。これは確率的アクタークリティックの性能を向上させなかった。COPDAC-Bは依然としてSAC-Bを非常に大きなマージンで上回り、それは次元が大きくなるにつれて大きくなった。

5.2. continuous reinforcement learning

2番目の実験では、標準的な強化学習ベンチマークである山車、振り子、2次元水たまりの世界の連続行動のバリエーションを検討した。我々の目的は、ガウス探索のもとで、確率的アクタークリティックと決定論的アクタークリティックのどちらが効率的かを見ることである。確率的アクタークリティック（SAC）アルゴリズムは、Degris et al.（2012a）のアクタークリティックアルゴリズムであり、このアルゴリズムは、マウンテンカーでの比較において、いくつかの漸進的アクタークリティック手法の中で最高の性能を示した。これは、特徴の線形結合πθ,y(s, -) ∼ N (θ >φ(s), exp(y >φ(s))) に基づくガウス型ポリシーを使用し、ポリシーの平均と分散の両方を適応させる。評論家は、時間差学習によって更新される同じ特徴を持つ線形値関数近似器V (s) = v >φ (s) を使用する。決定論的アルゴリズムはCOPDAC-Qに基づき、線形目標政策、μθ（s）=θ >φ（s） と固定幅ガウス行動政策、β（-|s）∼N（θ >φ（s）, σ2 β ）を用いています。批評家は再び、適合する行動-価値関数の基準として、線形価値関数V (s) = v >φ(s) を使用する。両者とも特徴量φ(s)は状態空間をタイルコーディングして生成される。また、先程と同じ行動政策βを用い、SACと同様に確率的政策πθ,y（s, -）を学習するオフポリシー確率的行為者批判アルゴリズム（OffPAC）との比較も行った。このアルゴリズムも、同じ批判者V (s) = v >φ(s) アルゴリズムと、Degris et al. (2012b) で説明された更新アルゴリズムを、λ = 0、αu = 0 で使用した。

すべてのアルゴリズムにおいて、エピソードは最大5000ステップで切り捨てられた。割引率は、山車と振り子ではγ=0.99、水たまりの世界ではγ=0.999であった。合法的な範囲外の行動には上限を設けた。ステップサイズパラメータに対してパラメータスイープを行い、分散は合法的範囲の1/2に初期化した。図2は、各アルゴリズムで最も性能の良いパラメータの性能を、30回の実行で平均化したものである。COPDAC-Qは3つの領域すべてにおいて、SACとOffPACの両方をわずかに上回った。

5.3. octopus arm

最後に、我々のアルゴリズムをタコ腕(Engel et al., 2005)のタスクでテストした。この課題は、タコの腕を模擬したものを制御し、ターゲットに当てることを学習するものである。腕は6つのセグメントからなり、回転するベースに取り付けられている。50個の連続状態変数（腕の上側と下側に沿った節のx,y位置/速度、基部の角度位置/速度）と、各節の3つの筋肉（背側、横側、中央）と基部の時計回りと反時計回りの回転を制御する20個の動作変数がある。目標は、腕のどの部分でもターゲットを叩くことである。報酬関数は腕と標的の距離の変化に比例する。エピソードは、ターゲットに当たるか（＋50の追加報酬あり）、300ステップ後に終了する。また、4つのセグメントからなる低次元のタコ腕に、確率的な政策勾配を適用した(Heess et al., 2012)。ここでは、6セグメントの高次元タコ腕に決定論的な政策勾配を直接適用する。政策μ(s)を表現するために、シグモイド多層パーセプトロン（隠れユニット8個、出力ユニットシグモイド）を用いて、COPDAC-Qアルゴリズムを適用した。アドバンテージ関数Aw(s, a)は互換関数近似（4.3節参照）、状態値関数V v (s)は第二多層パーセプトロン（隠れユニット40、出力ユニット線形）3 で表現した。10回の学習実行結果を図3に示すが、タコ足はすべてのケースで良い解に収束している。COPDAC-Qで学習させた8セグメントアームの動画も公開されています4。

6. discussion and related work

確率的政策勾配アルゴリズムを用いると，アルゴリズムが良い戦略に到達するにつれて，政策がより決定的になる．しかし，政策勾配∇θπθ(a|s)は平均値付近でより急激に変化するため，残念ながら確率的政策勾配の推定は難しくなります．実際，ガウス型政策N (μ, σ2 )に対する確率的政策勾配の分散は1/σ2に比例し (Zhao et al., 2012)，政策が決定論的になると無限大に増大します．この問題は，連続バンディット課題によって示されるように，高次元ではより深刻になる．確率的行為者批評家は，式 2 の確率的政策勾配を推定する．内積分 R A ∇θπθ(a|s)Qπ (s, a)da は高次元の行動空間をサンプリングすることにより計算される。一方、決定論的政策勾配は閉形式で即座に計算することができる。この決定論的行為者批評は，政策勾配の文脈ではQ-learning (Watkins and Dayan, 1992)に類似しているとみなすことができる．Q-learningは、貪欲な政策のノイズ版を実行しながら、政策によらず決定論的な貪欲政策を学習する。同様に、我々の実験ではCOPDAC-Qを用い、決定論的な政策、off-policyを学習し、その政策のノイズの多いバージョンを実行した。我々の実験では、オンポリシーとオフポリシーのアルゴリズムを比較しましたが、これは一見奇妙に見えるかもしれないことに注意してください。しかし，これはQ-learningとSarsaのどちらが効率的かを，それぞれのアルゴリズムによって学習された貪欲な政策を測定することによって問うことに類似している（Sutton and Barto, 1998）．

我々の行為者批評アルゴリズムは、モデルフリー、インクリメンタル、確率的勾配更新に基づいている。これらの方法は、モデルが未知で、データが豊富で、計算がボトルネックである場合に適している。これらの方法をバッチ／エピソード更新に拡張するのは原理的に簡単で、例えば、漸進的Q学習評論家の代わりにLSTDQ（Lagoudakis and Parr, 2003）を使用することができる。また，モデルベースの政策勾配法に関する文献も充実しており，主に決定論的で完全既知の遷移ダイナミクスに焦点をあてている（Werbos, 1990）．これらの方法は，遷移ダイナミクスが決定論的である場合，決定論的政策勾配と強く関連している．

行動値勾配が強化学習に有用なシグナルを与えることに気付いたのは我々が最初ではない．NFQCAアルゴリズム(Hafner and Riedmiller, 2011)は2つのニューラルネットワークを用いて、行為者と批判者をそれぞれ表現している。行為者は、式 7 のような更新を用いて、最初のニューラルネットで表現される政策を行為値勾配の方向に調整する。評論家は、第二のニューラルネットワークで表される行動値関数を、ニューラルフィットQ学習（近似値反復のための一括Qlearning更新）により更新する。しかし、そのcriticネットワークはactorネットワークと互換性がなく、criticが学習した局所最適値（収束すると仮定）がactorの更新とどのように相互作用するかは不明である。

7. conclusion

我々は決定論的政策勾配アルゴリズムのためのフレームワークを提示した。これらの勾配は、行動空間上で問題となる積分を回避し、確率的対応物よりも効率的に推定することができる。実際に、50個の連続行動次元を持つバンディットにおいて、決定論的アクター・クリティックは確率的対応策を数桁上回る性能を示し、20個の連続行動次元と50個の状態次元を持つ困難な強化学習問題を解決した。
