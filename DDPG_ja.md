深層強化学習による連続制御

Abstract

我々は、Deep Q-Learningの成功の基礎となるアイデアを連続的な行動領域に適応させる。我々は、連続的な行動空間上で動作することができる決定論的政策勾配に基づく、アクター批判的なモデルフリーアルゴリズムを提示する。同じ学習アルゴリズム、ネットワークアーキテクチャ、ハイパーパラメータを用いて、我々のアルゴリズムは、カートポールの振り上げ、器用な操作、脚式ロコモーション、自動車運転などの古典的問題を含む20以上のシミュレーション物理タスクを堅牢に解決する。このアルゴリズムは、領域とその導関数のダイナミクスに完全にアクセスできるプランニングアルゴリズムによって見出されるものに匹敵する性能を持つ方針を見出すことができる。さらに、多くのタスクにおいて、このアルゴリズムは、生のピクセル入力から直接、「エンド・ツー・エンド」で方針を学習できることを実証する。

1 introduction

人工知能の分野では、未処理の高次元感覚入力から複雑な課題を解決することが主要な目標の一つである。最近、感覚処理のための深層学習の進歩（Krizhevsky et al., 2012）と強化学習を組み合わせることによって大きな進展があり、その結果、入力に未処理のピクセルを使用して多くのアタリビデオゲームで人間レベルのパフォーマンスを実現できる「Deep Q Network」（DQN）アルゴリズム（Mnih et al., 2015）が誕生しました。そのために、アクション-バリュー関数を推定するために、ディープニューラルネットワーク関数近似器を使用しました。

しかし、DQNは高次元の観測空間の問題を解決する一方で、離散的で低次元の行動空間しか扱えない。多くのタスク、特に物理制御タスクは、連続的（実数値）かつ高次元の行動空間を持つ。DQNは、アクション-バリュー関数を最大化するアクションを見つけることに依存しており、連続値の場合、すべてのステップで反復最適化プロセスを必要とするため、連続ドメインに直接適用することはできない。

DQNのような深層強化学習手法を連続領域に適用するための明らかなアプローチは、アクション空間を単純に離散化することである。しかし、これには多くの限界があり、特に次元の呪いと呼ばれる、行動の数が自由度の数に対して指数関数的に増加する現象がある。例えば、人間の腕のような7自由度系で、各関節を最も粗く離散化したai∈{-k, 0, k}は、3 7 = 2187の次元を持つ行動空間となる。さらに、細かい動作制御が必要なタスクでは、より細かい離散化が必要となり、離散動作の数が爆発的に増えてしまう。このような大規模な行動空間は効率的に探索することが困難であり、このような状況でDQN的なネットワークをうまく学習させることは、おそらく困難である。さらに、行動空間の素朴な離散化は、多くの問題を解決するために不可欠である行動領域の構造に関する情報を無駄に捨ててしまう。

本研究では、高次元連続行動空間において政策を学習することができる、深い関数近似を用いたモデルフリーで政策に依存しない行為者批評アルゴリズムを提示する。我々の作品は決定論的政策勾配（DPG）アルゴリズム（Silver et al., 2014）（それ自体はNFQCA（Hafner & Riedmiller, 2011）に類似し、同様のアイデアは（Prokhorov et al., 1997）に見られる）に基づくものである。しかし、以下に示すように、ニューラル関数近似器を用いたこのアクター・クリティック法の素朴な適用は、困難な問題に対して不安定である。

ここでは、アクター・クリティックのアプローチと、近年のDeep Q Network (DQN) (Mnih et al., 2013; 2015)の成功からの洞察を組み合わせています。DQN以前は、一般に、大きな非線形関数近似器を用いた価値関数の学習は困難であり、不安定であると考えられていた。DQNは、2つの革新的な技術により、このような関数近似器を用いた価値関数を安定かつ頑健に学習することができる。1.サンプル間の相関を最小化するために、リプレイバッファからのサンプルでネットワークをオフポリシーで学習する。2.時間差バックアップ中に一貫したターゲットを与えるために、ターゲットQネットワークでネットワークを学習する。本研究では、深層学習における最近の進歩であるバッチ正規化（Ioffe & Szegedy, 2015）と共に、同じアイデアを利用する。

我々の手法を評価するために、我々は複雑な多関節運動、不安定で豊富な接触ダイナミクス、および歩行挙動を含む、様々な挑戦的な物理制御問題を構築した。これらの中には、カートポールの振り上げ問題などの古典的な問題や、多くの新しい領域が含まれています。ロボット制御の長年の課題は、映像のような生の感覚入力から直接行動方針を学習することである。そこで、我々はシミュレータ内に固定視点カメラを設置し、低次元の観測値（関節角度など）とピクセルからの直接入力の両方を用いて、全てのタスクを試みた。

Deep DPG（DDPG）と呼ぶ我々のモデルフリーアプローチは、同じハイパーパラメータとネットワーク構造を用いて、直交座標や関節角などの低次元観測値を用いた全てのタスクに対して競争力のある政策を学習することができる。また、多くの場合、ピクセルから直接、ハイパーパラメータとネットワーク構造を一定に保ったまま、良い政策を学習することができる1 。

このアプローチの主な特徴は、そのシンプルさである。単純なアクター・クリティック・アーキテクチャと学習アルゴリズムのみを必要とし、「動く部分」が非常に少ないため、実装が容易で、より難しい問題やより大きなネットワークに拡張することが可能である。物理制御問題については、シミュレーションされたダイナミクスとその導関数に完全にアクセスできるプランナー(Tassa et al., 2012)によって計算されたベースラインと比較しました（補足情報参照）。興味深いことに、DDPGはピクセルから学習した場合でも、プランナーの性能を超える政策を見つけることができる場合があります（プランナーは常に基礎となる低次元状態空間上で計画を立てます）。

2 background

我々は、離散的なタイムステップで環境Eと相互作用するエージェントからなる標準的な強化学習のセットアップを考慮する。各タイムステップ t において、エージェントは観測値 xt を受け取り、行動 at をとり、スカラー報酬 rt を受け取る。ここで考慮するすべての環境において、行動は実数値 ∈ IRN である。一般に、環境は部分的に観測され、観測と行動の組 st = (x1, a1, ..., at-1, xt) の全履歴が状態を記述するために必要とされるかもしれない。ここでは、環境は完全に観測されていると仮定しているので、st = xt とする。

エージェントの行動は、状態を行動に対する確率分布π : S → P(A)に写像する方針πによって定義される。環境 E も確率的である。状態空間 S, 行動空間 A = IRN, 初期状態分布 p(s1), 遷移動力学 p(st+1|st, at), 報酬関数 r(st, at) を持つマルコフ決定過程としてモデル化する。

状態からのリターンは、割引係数γ∈[0, 1]を用いた割引後の将来の報酬 Rt = PT i=t γ (i-t) r(si , ai) の総和として定義される。なお、リターンは選択された行動、ひいては政策πに依存し、確率的である可能性がある。強化学習における目標は、開始分布J = Eri,si∼E,ai∼π [R1]から期待リターンを最大化するような政策を学習することである。ある政策πに対する割引状態訪問分布をρ πとする．

行動値関数は多くの強化学習アルゴリズムで用いられている．これは，状態 st で行動を起こし，その後政策πに従った後の期待リターンを表す．
(((1))) 

強化学習の多くのアプローチは、ベルマン方程式として知られる再帰的な関係を利用する： 
(((2))) 

目標政策が決定論的であれば、それを関数 µ : S ← A として記述し、内部期待値を回避することができる： 
(((3))) 

期待値は環境のみに依存する。つまり、別の確率的行動政策βから生成される遷移を用いて、政策によらないQµを学習することが可能である。よく使われるオフポリシーのアルゴリズムであるQ学習(Watkins & Dayan, 1992)は、貪欲政策μ(s) = arg maxa Q(s, a)を使用する。我々はθ Qをパラメータとする関数近似器を考え、損失を最小化することで最適化する：
（（4））

ここで、
（（5））
である。

ytもθ Qに依存するが、これは通常無視される。

価値関数や行動価値関数の学習に大きな非線形関数近似を用いることは、理論的な性能保証が不可能であり、現実的に学習が不安定になる傾向があるため、従来はしばしば避けられてきた。最近、(Mnih et al., 2013; 2015)は、関数近似器として大規模なニューラルネットワークを有効に利用するために、Q-learningアルゴリズムを適応させた。彼らのアルゴリズムは、ピクセルからアタリゲームをプレイすることを学習することができた。Q-learningをスケールアップするために、彼らは2つの大きな変更を導入しました：リプレイバッファの使用、およびytを計算するための別のターゲットネットワークです。我々はこれらをDDPGの文脈で採用し、次のセクションでその実装を説明する。

3 algorithm

連続空間において貪欲なポリシーを見つけるにはタイムステップ毎にatの最適化が必要であるため、Q-learningを連続行動空間に素直に適用することはできない。この最適化は、大規模で制約のない関数近似と非自明な行動空間では遅すぎて実用的でない。代わりに、ここでは、DPGアルゴリズム（Silver et al.、2014）に基づくアクター・クリティック・アプローチを使用しました。

DPGアルゴリズムは、状態を特定のアクションに決定論的にマッピングすることによって現在のポリシーを指定するパラメータ化されたアクター関数µ(s|θ µ)を保持する。批評家Q(s, a)はQ-learningと同様にベルマン方程式を用いて学習される。アクターは、アクターパラメーターに関して、開始分布 J からの期待リターンに連鎖法則を適用することによって更新される：
(((6))) 

Silver ら (2014) は，これがポリシーの勾配であることを証明した 2．

Q 学習と同様に，非線形関数近似器を導入すると，収束が保証されなくなる．しかし，大きな状態空間を学習し汎化するためには，このような近似器が必要不可欠であると思われる．NFQCA (Hafner & Riedmiller, 2011) はDPGと同じ更新則を用いるが、ニューラルネットワークの関数近似を用いるため、安定性のためにバッチ学習を用いるが、これは大規模ネットワークでは実用的でない。NFQCAのミニバッチ版では、大規模ネットワークへの拡張に必要な更新ごとのポリシーのリセットを行わないため、オリジナルのDPGと同等であり、ここで比較する。我々の貢献は、DQNの成功に触発され、DPGに修正を加えることで、ニューラルネットワーク関数近似器を用いて、大きな状態および行動空間でオンライン学習することを可能にすることです。我々はこのアルゴリズムをDeep DPG (DDPG、アルゴリズム1)と呼ぶことにする。

強化学習にニューラルネットワークを用いる場合の課題の1つは、ほとんどの最適化アルゴリズムが、サンプルが独立かつ同一に分布していると仮定していることです。明らかに、サンプルが環境中を順次探索して生成される場合、この仮定はもはや成り立たない。さらに、ハードウェアの最適化を効率的に利用するためには、オンラインではなく、ミニバッチで学習することが不可欠である。

DQNと同様に、これらの問題に対処するために、リプレイバッファを使用しました。リプレイバッファは有限サイズのキャッシュRであり、探索方針に従って環境から遷移をサンプリングし、そのタプル（st, at, rt, st+1）をリプレイバッファに格納する。リプレイバッファが満杯になると、最も古いサンプルは廃棄された。各タイムステップにおいて、バッファからミニバッチを一様にサンプリングすることで、アクターとクリティックが更新される。DDPGはオフポリシーアルゴリズムであるため、リプレイバッファを大きくすることができ、アルゴリズムが相関のない遷移の集合に渡って学習することで利益を得ることができるようになります。

Q学習（式4）をニューラルネットワークで直接実装すると、多くの環境で不安定であることが判明した。更新されるネットワークQ（s, a|θ Q）は目標値（式5）の計算にも使われるため、Qの更新は発散しやすいのである。我々の解決策は(Mnih et al., 2013)で使用された目標ネットワークに似ているが、アクター・クリティック用に修正し、重みを直接コピーするのではなく、「ソフト」目標更新を使用することである。我々は、目標値を計算するために使用される行為者と批判者のネットワーク、Q0 (s, a|θ Q0 ) とμ 0 (s|θ μ 0 ) をそれぞれコピーして作成する。そして、これらの目標ネットワークの重みは、学習されたネットワークにゆっくりと追従させることによって更新される：τ 0 ← τθ + (1 - τ )θ 0 with τ 1. これは、目標値がゆっくりと変化するように拘束されることを意味し、学習の安定性を大きく向上させる。この単純な変更により、比較的不安定な行動値関数の学習問題を、ロバストな解が存在する教師あり学習の場合に近づけることができる。我々は、発散することなく一貫してcriticを学習させるためには、安定した目標yiを持つために、目標µ0とQ0の両方を持つことが必要であることを見出した。これは、ターゲットネットワークが値推定の伝搬を遅らせるため、学習が遅くなる可能性がある。しかし、実際には、このことは学習の安定性によって大きく相殺されることがわかった。

低次元の特徴ベクトル観測から学習するとき、観測の異なるコンポーネントは異なる物理単位（例えば、位置と速度）を持ち、その範囲は環境間で異なるかもしれない。これはネットワークが効果的に学習することを困難にし、状態値のスケールが異なる環境にわたって汎化するハイパーパラメータを見つけることを困難にする可能性がある。

この問題に対する一つのアプローチは、環境とユニット間で類似した範囲になるように手動で特徴をスケーリングすることである。我々は、バッチ正規化（Ioffe & Szegedy, 2015）と呼ばれる深層学習からの最近の技術を適応することでこの問題に対処する。この技術は、ミニバッチ内のサンプル間で各次元を正規化し、単位平均と単位分散を持つようにする。さらに、テスト中（我々の場合、探索または評価中）に正規化に使用するために、平均と分散の実行平均を維持する。ディープネットワークでは、各層が白くなった入力を受け取るようにすることで、学習中の共分散シフトを最小化するために使用される。低次元の場合、行動入力の前に、状態入力とμネットワークの全層、Qネットワークの全層に対して一括正規化を行った（ネットワークの詳細は補足資料に記載）。一括正規化により、ユニットが設定範囲内にあることを手動で確認する必要がなく、異なるタイプのユニットを持つ多くの異なるタスクで効果的に学習することができた。

連続行動空間における学習の大きな課題は探索である。DDPGのようなオフポリシーアルゴリズムの利点は、探索の問題を学習アルゴリズムから独立して扱うことができることである。我々は、ノイズ過程 N からサンプリングしたノイズをアクターポリシーに加えることで、探索ポリシー µ 0 を構築した 
(((7))) 。

Nは環境に合わせて選択することができる。補足資料に詳述されているように、我々はOrnstein-Uhlenbeck過程（Uhlenbeck & Ornstein, 1930）を用いて、慣性を伴う物理制御問題における探索効率のために時間的に相関した探索を生成しました（自己相関ノイズの同様の使用は（Wawrzynski, 2015）に導入されています）。

4 results

我々は、アルゴリズムをテストするために、様々な難易度の物理環境をシミュレートした。この環境には、カートポールのような古典的な強化学習環境、グリッパーのような困難で高次元のタスク、パック打ち（カナダ）のような接触を伴うタスク、チーター（Wawrzynski、2009）のような運動タスクが含まれている。また、チーター以外のすべての領域において、動作させた関節にトルクを与えている。これらの環境は、MuJoCo (Todorov et al., 2012)を用いてシミュレートされた。図1はタスクで使用された環境の一部のレンダリングである（補足には環境の詳細が含まれており、https://goo.gl/J4PIAz で学習されたポリシーの一部を見ることができる）。

全てのタスクにおいて、低次元の状態記述（関節角度や位置など）と高次元の環境レンダリングの両方を用いて実験を行った。DQN (Mnih et al., 2013; 2015) のように、高次元環境で問題をほぼ完全に観測可能にするために、我々はアクションリピートを使用した。エージェントの各タイムステップに対して、我々はシミュレーションを3タイムステップし、その都度エージェントのアクションとレンダリングを繰り返しました。したがって、エージェントに報告される観測は、9つの特徴マップ（3つのレンダリングのそれぞれのRGB）を含み、エージェントはフレーム間の差分を用いて速度を推測することができます。フレームは64x64ピクセルにダウンサンプリングされ、8ビットRGB値は[0, 1]にスケーリングされた浮動小数点に変換されました。ネットワーク構造とハイパーパラメータの詳細については補足情報を参照されたい．

学習中，探査ノイズを用いないテストにより，定期的にポリシーの評価を行った．図2は、いくつかの環境に対する性能曲線を示している。また、我々のアルゴリズムの構成要素（すなわち、ターゲットネットワークやバッチ正規化）を取り除いた結果も報告する。すべてのタスクで良好な結果を得るためには、これら両方の追加が必要である。特に、DPGを用いたオリジナル作品のように、ターゲットネットワークを用いない学習は、多くの環境において非常に貧弱である。

意外なことに、いくつかの簡単なタスクでは、ピクセルからポリシーを学習することは、低次元状態記述子を用いた学習と同じくらい高速である。これは、行動の繰り返しが問題を単純化しているためと思われる。また、畳み込み層が状態空間を容易に分離できる表現を提供し、上位層が素早く学習できるようにしたためとも考えられる。

表1は、すべての環境におけるDDPGの性能をまとめたものである（結果は5つのレプリカの平均値である）。スコアは2つのベースラインを用いて正規化した。1つ目のベースラインは、有効なアクション空間上の一様分布からアクションをサンプリングするナイーブポリシーからの平均リターンである。もう1つは、物理モデルとその導関数に完全にアクセスできるプランニングベースのソルバーであるiLQG (Todorov & Li, 2005)である。DDPGは多くのタスクで良好な政策を学習することができ，多くの場合，ピクセルから直接学習した場合でもiLQGが発見した政策よりも優れた政策を学習するレプリカが存在する．

正確な値の推定を学習することは困難な場合があります。例えばQ-learningは値を過大評価する傾向がある(Hasselt, 2010)。我々はDDPGの推定値を、学習後にQで推定された値とテストエピソードで見られる真のリターンを比較することで経験的に検証した。図3は、単純なタスクでは、DDPGは系統的な偏りなく正確にリターンを推定していることを示している。難しいタスクではQの推定値は悪くなるが、DDPGはまだ良い政策を学習することができる。また、本アプローチの一般性を示すために、アクセル、ブレーキ、ステアリングを操作するレースゲームであるTorcsを取り込みました。Torcsは以前，他の政策学習アプローチのテストベッドとして使用された(Koutn´ık et al., 2014b)．我々は物理タスクと同一のネットワークアーキテクチャと学習アルゴリズムのハイパーパラメータを使用したが、関係する時間スケールが非常に異なるため、探索のためのノイズプロセスを変更した。低次元とピクセルからの両方で、いくつかのレプリカはトラックを一周することができる合理的なポリシーを学習することができましたが、他のレプリカは賢明なポリシーを学習することができませんでした。

5 related work

オリジナルのDPGの論文では、タイルコーディングと線形関数近似を用いたおもちゃの問題でアルゴリズムを評価しました。この論文では、オフポリシーDPGが、オンポリシーおよびオフポリシーの確率的アクター批判に対して、データ効率の点で優れていることを実証しています。また、多関節のタコの腕が四肢のどの部分でも標的を攻撃しなければならないという、より困難な課題も解決しています。しかし、この論文では、今回のように大規模で高次元の観測空間へのアプローチのスケーリングを実証していない。

本研究で探求されたような標準的な政策探索手法は、困難な問題にスケールするためには単に脆弱であるとしばしば仮定されてきた(Levine et al., 2015)。標準的な政策探索は，複雑な環境ダイナミクスと複雑な政策を同時に扱うため，困難であると考えられている．実際，アクター・クリティックや政策最適化アプローチを用いた過去の研究のほとんどは，より困難な問題へのスケールアップが困難であった (Deisenroth et al., 2013)．一般に，これは学習の不安定性によるものであり，問題に対する進歩がその後の学習更新によって破壊されるか，あるいは学習が遅すぎて実用的でないかのどちらかである．

モデルフリー政策探索に関する最近の研究は，それが以前考えられていたほど壊れやすいものではないことを実証している．Wawrzynski (2009); Wawrzy´ nski & Tanwani (2013) はリプレイバッファを用いたアクタークリティックのフレームワークにおいて確率的な政策を学習させることに成功した．我々の研究と同時期にBalduzzi & Ghifary (2015)は、∂Q/∂aを明示的に学習する「deviator」ネットワークでDPGアルゴリズムを拡張している。しかし、彼らは2つの低次元ドメインでしか訓練していない。Heessら(2015)はSVG(0)を導入し，これもQ-criticを用いるが，確率的なポリシーを学習する．DPGはSVG(0)の決定論的な限界と考えることができる。DPGをスケーリングするためにここで説明した技術は、再パラメトリゼーショントリックを用いることで確率的政策にも適用可能である(Heess et al., 2015; Schulman et al., 2015a)．

別のアプローチである信頼領域政策最適化（TRPO）（Schulman et al., 2015b）は、問題を最適制御フェーズと教師フェーズに分解することなく、直接、確率的ニューラルネットワーク政策を構築する。この方法は、新しいポリシーが既存のポリシーから大きく乖離しないように更新を制約しながら、ポリシーパラメータを慎重に選択して更新を行うことで、単調に近いリターンの改善をもたらすものである。この方法は行動価値関数を学習する必要がないため，（おそらくその結果として）データ効率が大幅に低下するように見える．

行為者批判アプローチの課題に対抗するために、ガイド付き政策探索（GPS）アルゴリズムによる最近の研究（例えば、（Levine et al, 2015))は，問題を比較的容易に解決できる3つのフェーズに分解する．まず，全状態観測を用いて1つ以上の名目軌道の周りのダイナミクスの局所線形近似を作成し，次に最適制御を用いてこれらの軌道に沿った局所線形最適政策を見つけ，最後に最適化軌道の状態-行動マッピングを再現するために教師あり学習を用いて複雑で非線形政策（例えば，深層神経ネットワーク）を訓練する．

このアプローチは、データ効率などいくつかの利点があり、視覚を用いた様々な実世界のロボット操作タスクへの適用に成功している。これらのタスクにおいて、GPSは我々のものと同様の畳み込み政策ネットワークを用いているが、2つの顕著な例外がある。1.空間ソフトマックスを用い、視覚的特徴の次元を各特徴マップの単一（x、y）座標に削減すること、2.ネットワーク内の最初の完全接続層でロボットの構成に関する低次元の状態情報を直接受け取ることもポリシーに含まれる。この両者は、おそらくアルゴリズムのパワーとデータ効率を向上させ、DDPGフレームワーク内で容易に利用することができる。

PILCO (Deisenroth & Rasmussen, 2011) はガウス過程を使用して、ダイナミクスのノンパラメトリックな確率的モデルを学習します。この学習されたモデルを用いて、PILCOは解析的な政策勾配を計算し、多くの制御問題において印象的なデータ効率を達成する。しかし、計算量が多いため、PILCOは「高次元問題には実用的ではない」（Wahlstromら、¨2015）。強化学習を大規模な高次元ドメインに拡張するためには、深い関数近似が最も有望なアプローチであると思われます。

Wahlstromら（2015）は、モデル予測制御¨とともに深層力学モデルネットワークを用いて、ピクセル入力から振り子振り上げ課題を解決しました。彼らは微分可能な前方モデルを学習し、学習した潜在空間に目標状態を符号化した。彼らは学習したモデルに対してモデル予測制御を行い、目標に到達するための方針を見出す。しかし、このアプローチはアルゴリズムに示すことができる目標状態を持つドメインにのみ適用可能である。

近年，進化的アプローチにより，圧縮された重みのパラメトリック（Koutn´ık et al., 2014a）や教師なし学習（Koutn´ık et al., 2014b）を用いて，進化した重みの次元を減らし，ピクセルからトークの競争政策を学ぶことが行われている．これらのアプローチが他の問題に対してどの程度一般化できるかは不明である。

6 conclusion

この研究は、深層学習と強化学習における最近の進歩からの洞察を組み合わせたもので、その結果、生のピクセルを観測に用いた場合でも、連続的なアクション空間を持つ様々な領域にわたって困難な問題を頑健に解決するアルゴリズムが実現されました。多くの強化学習アルゴリズムと同様に、非線形関数近似器の使用は収束の保証を無効にしますが、我々の実験結果は、環境間で修正を必要としない安定した学習を実証しています。

興味深いことに、すべての実験において、アタリ領域で解を求めるためにDQN学習が用いた経験値よりも大幅に少ないステップしか用いていない。これは、DQNがAtariの解を求めるのに必要なステップ数より20ステップも少ない。このことは，より多くのシミュレーション時間があれば，DDPGはここで検討した問題よりもさらに難しい問題を解ける可能性があることを示唆しています．

我々のアプローチにはいくつかの限界が残っています。最も重要なことは、多くのモデルフリー強化アプローチと同様に、DDPGは解を見つけるために多くの訓練エピソードを必要とすることである。しかし、我々は、ロバストなモデルフリーアプローチが、これらの限界を攻略するような大規模システムの重要な構成要素になると考えている（Glascher et al.、2010）。

補足情報 深層強化学習による連続制御

7 experiment details

ニューラルネットワークのパラメータの学習には Adam (Kingma & Ba, 2014) を用い、学習率は俳優と批評家にそれぞれ 10-4 と 10-3 を設定した。Qについては、10-2のL2重み減衰を含み、γ = 0.99の割引係数を使用した。ソフトターゲット更新にはτ=0.001を用いた。ニューラルネットワークは全ての隠れ層に整流非線形(Glorot et al., 2011)を使用した。アクターの最終出力層は tanh 層であり、行動を束縛するものであった。低次元ネットワークはそれぞれ400と300のユニットを持つ2つの隠れ層を持っていた（≒130,000パラメータ）。ピクセルから学習する場合、我々は3つの畳み込み層（プーリングなし）を使用し、各層に32のフィルタを設定した。この後、200ユニット（≒430,000パラメータ）の2つの完全連結層が続く。また、批評家と俳優の最終層の重みとバイアスは、低次元と画素のそれぞれについて、一様分布 [-3 × 10-3 , 3 × 10-3 ] と [3 × 10-4 , 3 × 10-4 ] から初期化された。これは、政策と価値の推定値の初期出力がゼロに近いことを保証するためである。他の層は一様分布 [- √ 1 f , √ 1 f ]（fは層のファンイン）から初期化された．アクションは完全に接続された層まで含まれなかった．ミニバッチサイズは低次元問題では64、ピクセル問題では16で学習した。リプレイバッファのサイズは106である。

また、このような物理的な環境下での探索のために、時間的な相関のあるノイズを使用した。Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) を用い、θ = 0.15 と σ = 0.2 とした。Ornstein-Uhlenbeck過程は摩擦を伴うブラウン粒子の速度をモデル化しており、0を中心とした時間的に相関のある値になる。

8 planning algorithm

我々のプランナーはモデル予測制御(Tassa et al., 2012)として実装されています：すべての時間ステップにおいて、システムの真の状態から出発して、軌道最適化の単一の反復を実行します（iLQG, (Todorov & Li, 2005)を使用）。軌道最適化は250msから600msの間に計画され、モデル予測制御のように世界のシミュレーションが展開されるにつれて、この計画地平は後退していきます。

iLQGの反復は、ノミナル軌道を決定する直前のポリシーの初期展開から始まる。シミュレーションされたダイナミクスのサンプルを繰り返し使用し、軌道の各ステップにおけるダイナミクスの線形展開と、コスト関数の二次展開を近似する。この一連の局所線形二次モデルを使って、価値関数を公称軌道に沿って時間的に逆積分する。このバックパスの結果、総コストを減少させる行動シーケンスの推定上の修正が行われる。この方向に対して、ダイナミクスを前方に積分することにより、行動列の空間で無微分線探索を行い（フォワードパス）、最適な軌道を選択する。次のiLQGの反復をウォームスタートするために、このアクションシーケンスを保存し、シミュレータで最初のアクションを実行します。この結果、新しい状態が得られ、これが次の軌道最適化の繰り返しにおける初期状態として使用される。

9 environment details

9.1 torcs environment

Torcs環境では、トラック方向に投影された車の速度に対して各ステップで正の報酬を与え、衝突した場合に-1のペナルティを与える報酬関数を使用した。500フレームを経過しても軌道に沿って前進しない場合、エピソードは終了した。

9.2 mujoco environmenta

物理制御タスクでは、ステップ毎にフィードバックを与える報酬関数を使用した。すべてのタスクにおいて、報酬には小さな行動コストが含まれている。目標状態が静的なタスク（例：振り子の振り上げ、リーチング）に対しては、目標状態までの距離に応じて滑らかに変化する報酬を与え、場合によっては、目標状態から小さな半径内に入ったときに追加の正の報酬を与えるようにした。把持・操作タスクでは、ペイロードに向かう動きを促す項と、ペイロードをターゲットに移動させることを促す第2項を持つ報酬を使用しました。運動タスクでは、ホッピングではなくスムーズな歩行を促すために、前進動作に報酬を与え、ハードインパクトにペナルティを与えます（Schulman et al.，2015b）。さらに、我々は、高さと胴体の角度（walker2dの場合）の単純な閾値によって決定された転倒に対する負の報酬と早期終了を使用しました。

